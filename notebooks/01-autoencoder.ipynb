{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image compression for acoustic transmissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'andrea munafo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook investigates some image compression methods using neural networks.\n",
    "\n",
    "The point is to obtain a dimentionality reduction that can be supported over very bandwidth constrained media, such as acoustics.\n",
    "\n",
    "This description of autoencoders is based on [Understanding Variational Autoencoders](\n",
    "https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73), on [Autoencoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) and on [Tutorial on variational autoencoders](https://arxiv.org/abs/1606.05908).\n",
    "\n",
    "and is reported here to provide a description for the implementation reported below.\n",
    "\n",
    "All implementations are applied to the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is dimentionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is the process of reducing the number of features that describe some data.\n",
    "\n",
    "\n",
    "This reduction can be done, either selecting specific features or by reducing to a new set of features created from the original ones (feature extraction).\n",
    "Applications range from data visualisation, data storage, heavy computation and communications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s call encoder the process that produce the “new features” representation from the “old features” representation (by selection or by extraction) and decoder the reverse process. \n",
    "\n",
    "Dimensionality reduction can then be interpreted as data compression where the encoder compress the data (from the initial space to the encoded space, also called latent space) whereas the decoder decompress them.\n",
    "Of course, depending on the initial data distribution, the latent space dimension and the encoder definition, this compression can be lossy, meaning that a part of the information is lost during the encoding process and cannot be recovered when decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![encoding-decoding](./pics/encoding-decoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem can be formulated as finding the optimal encoder $e$ in the set of possible encoders $E$, and the optimal decoder $d$, in the set of possible dencoders $D$, such that:\n",
    "\n",
    "$(e*,d*) = argmin f(x, d(e(x))$\n",
    "       \n",
    "where $e \\in E$, $d \\in D$, and $f(.)$ is the function that defines the reconstruction error measure between the input data $x$ and the encoded-decoded data $d(e(x))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to solve this problem is using Principal Components Analysis (PCA).\n",
    "\n",
    "The idea of PCA is to build $n_e$ new features that are linear combinations of the $n_d$ original features.\n",
    "This is done so that the projections of the original data on the subspace defined by these new features are as close as possible to the initial data (in term of euclidean distance).\n",
    "This is shown in the figure below:\n",
    "\n",
    "![pca](./pics/pca-ex.png)\n",
    "\n",
    "For example, the encoded version of point B is obtained as the projection of B onto the line that minimises the distance from all the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that, the unitary eigenvectors corresponding to the $n_e$ greatest eigenvalues (in norm) of the covariance features matrix C are orthogonal (or can be chosen to be so) and define the best subspace of dimension $n_e$ to project data on with minimal error of approximation. \n",
    "These $n_e$ eigenvectors can then be chosen as the new features.\n",
    "In the PCA context, the problem of dimension reduction can be expressed as an eigenvalue/eigenvector problem. \n",
    "Moreover, it can also be shown that the decoder matrix is the transposed of the encoder matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding and decoding matrices obtained with PCA define one of the solutions. \n",
    "However, several basis can be chosen to describe the same optimal subspace and, so, several encoder/decoder pairs can give the optimal reconstruction error.\n",
    "\n",
    "Now, let’s assume that both the encoder and the decoder are deep and non-linear. In such case, the more complex the architecture is, the more the autoencoder can proceed to a high dimensionality reduction while keeping reconstruction loss low. Intuitively, if our encoder and our decoder have enough degrees of freedom, we can reduce any initial dimensionality to 1. Indeed, an encoder with “infinite power” could theoretically takes our N initial data points and encodes them as 1, 2, 3, … up to N (or more generally, as N integer on the real axis) and the associated decoder could make the reverse transformation, with no loss during the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we should however keep two things in mind. First, an important dimensionality reduction with no reconstruction loss often comes with a price: the lack of interpretable and exploitable structures in the latent space (lack of regularity). Second, most of the time the final purpose of dimensionality reduction is not to only reduce the number of dimensions of the data but to reduce this number of dimensions while keeping the major part of the data structure information in the reduced representations. For these two reasons, the dimension of the latent space and the “depth” of autoencoders (that define degree and quality of compression) have to be carefully controlled and adjusted depending on the final purpose of the dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept is shown in the picture below. We would like to reduce the dimentionality, while keeping some of the structure of the data.\n",
    "\n",
    "![pca](./pics/encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "pathlib.Path(\"../results/01-autoencoder/ae\").mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(\"../results/01-autoencoder/convae\").mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(\"../results/01-autoencoder/vae\").mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(\"../saved-mdls/01-autoencoder\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toImg(x, mu=0.5, std=1):\n",
    "    \"\"\"Converts x to an image shape. It works for batches of inputs.\"\"\"\n",
    "    x = mu * (x + std)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some transforms to normalise the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mean = 0.1307\n",
    "ds_std = 0.3081\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((ds_mean,), (ds_std,)) # The first tuple (0.5, 0.5, 0.5) is the mean for all three channels and the second (0.5, 0.5, 0.5) is the standard deviation for all three channels.\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_ds = MNIST('./data', train=True, transform=img_transform, download=True)\n",
    "valid_ds = MNIST('./data', train=False, transform=img_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_ds.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining an Autoencoder model is not difficult. The following class shows one potential option as a combination of Linear and ReLU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(64, 12), \n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(12, 3))        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(128, 28 * 28), \n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.5602\n",
      "epoch [2/100], loss:0.5473\n",
      "epoch [3/100], loss:0.5371\n",
      "epoch [4/100], loss:0.5408\n",
      "epoch [5/100], loss:0.4876\n",
      "epoch [6/100], loss:0.4854\n",
      "epoch [7/100], loss:0.4918\n",
      "epoch [8/100], loss:0.5050\n",
      "epoch [9/100], loss:0.5683\n",
      "epoch [10/100], loss:0.4808\n",
      "epoch [11/100], loss:0.4911\n",
      "epoch [12/100], loss:0.4622\n",
      "epoch [13/100], loss:0.5145\n",
      "epoch [14/100], loss:0.5030\n",
      "epoch [15/100], loss:0.4437\n",
      "epoch [16/100], loss:0.5159\n",
      "epoch [17/100], loss:0.5257\n",
      "epoch [18/100], loss:0.4939\n",
      "epoch [19/100], loss:0.4832\n",
      "epoch [20/100], loss:0.4450\n",
      "epoch [21/100], loss:0.4416\n",
      "epoch [22/100], loss:0.4433\n",
      "epoch [23/100], loss:0.4908\n",
      "epoch [24/100], loss:0.4583\n",
      "epoch [25/100], loss:0.4830\n",
      "epoch [26/100], loss:0.4803\n",
      "epoch [27/100], loss:0.4719\n",
      "epoch [28/100], loss:0.4317\n",
      "epoch [29/100], loss:0.4332\n",
      "epoch [30/100], loss:0.5100\n",
      "epoch [31/100], loss:0.4836\n",
      "epoch [32/100], loss:0.4686\n",
      "epoch [33/100], loss:0.4797\n",
      "epoch [34/100], loss:0.5120\n",
      "epoch [35/100], loss:0.4823\n",
      "epoch [36/100], loss:0.4763\n",
      "epoch [37/100], loss:0.4901\n",
      "epoch [38/100], loss:0.4556\n",
      "epoch [39/100], loss:0.4851\n",
      "epoch [40/100], loss:0.4474\n",
      "epoch [41/100], loss:0.4293\n",
      "epoch [42/100], loss:0.4440\n",
      "epoch [43/100], loss:0.4943\n",
      "epoch [44/100], loss:0.5166\n",
      "epoch [45/100], loss:0.5518\n",
      "epoch [46/100], loss:0.5341\n",
      "epoch [47/100], loss:0.5265\n",
      "epoch [48/100], loss:0.5320\n",
      "epoch [49/100], loss:0.4872\n",
      "epoch [50/100], loss:0.4489\n",
      "epoch [51/100], loss:0.5037\n",
      "epoch [52/100], loss:0.4680\n",
      "epoch [53/100], loss:0.4607\n",
      "epoch [54/100], loss:0.4225\n",
      "epoch [55/100], loss:0.4512\n",
      "epoch [56/100], loss:0.5028\n",
      "epoch [57/100], loss:0.4769\n",
      "epoch [58/100], loss:0.5159\n",
      "epoch [59/100], loss:0.4600\n",
      "epoch [60/100], loss:0.4927\n",
      "epoch [61/100], loss:0.4544\n",
      "epoch [62/100], loss:0.4826\n",
      "epoch [63/100], loss:0.5090\n",
      "epoch [64/100], loss:0.5271\n",
      "epoch [65/100], loss:0.5107\n",
      "epoch [66/100], loss:0.4235\n",
      "epoch [67/100], loss:0.4578\n",
      "epoch [68/100], loss:0.4514\n",
      "epoch [69/100], loss:0.4575\n",
      "epoch [70/100], loss:0.4789\n",
      "epoch [71/100], loss:0.4688\n",
      "epoch [72/100], loss:0.5022\n",
      "epoch [73/100], loss:0.4651\n",
      "epoch [74/100], loss:0.4402\n",
      "epoch [75/100], loss:0.4553\n",
      "epoch [76/100], loss:0.5108\n",
      "epoch [77/100], loss:0.4504\n",
      "epoch [78/100], loss:0.4580\n",
      "epoch [79/100], loss:0.5241\n",
      "epoch [80/100], loss:0.4497\n",
      "epoch [81/100], loss:0.4625\n",
      "epoch [82/100], loss:0.4352\n",
      "epoch [83/100], loss:0.5229\n",
      "epoch [84/100], loss:0.5016\n",
      "epoch [85/100], loss:0.4678\n",
      "epoch [86/100], loss:0.4940\n",
      "epoch [87/100], loss:0.4866\n",
      "epoch [88/100], loss:0.5085\n",
      "epoch [89/100], loss:0.4982\n",
      "epoch [90/100], loss:0.4447\n",
      "epoch [91/100], loss:0.4906\n",
      "epoch [92/100], loss:0.4760\n",
      "epoch [93/100], loss:0.4070\n",
      "epoch [94/100], loss:0.4130\n",
      "epoch [95/100], loss:0.4456\n",
      "epoch [96/100], loss:0.4077\n",
      "epoch [97/100], loss:0.4823\n",
      "epoch [98/100], loss:0.4681\n",
      "epoch [99/100], loss:0.4975\n",
      "epoch [100/100], loss:0.4695\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for x, y in train_dl:\n",
    "        x = x.view(x.size(0), -1).to(device)  # resize to be a vector bsx28*28\n",
    "#        x = Variable(x).cuda()         \n",
    "\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.data))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        ipic = toImg(x.cpu().data)\n",
    "        opic = toImg(output.cpu().data)\n",
    "\n",
    "        save_image(opic, '../results/ae/image_{}_o.png'.format(epoch))\n",
    "        save_image(ipic, '../results/ae/image_{}_i.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done training. We can save the model so that we can use it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Autoencoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# As suggested in https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "torch.save(model, '../saved-mdls/01-autoencoder/autoencoder_{}e.pt'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using and validating the simple autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class must be defined somewhere\n",
    "model = torch.load('../saved-mdls/01-autoencoder/autoencoder.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, (x, _) in enumerate(valid_dl):\n",
    "    x = x.view(x.size(0), -1)\n",
    "    encoding = model.encode(x)\n",
    "    decoding = model.decode(encoding)\n",
    "    \n",
    "    # save (some) results\n",
    "    if count % 10 == 0:\n",
    "        ipic = to_img(x.cpu().data)    \n",
    "        opic = to_img(decoding.cpu().data)\n",
    "        save_image(ipic, '../results/01-autoencoder/ae/val-{}-enc.png'.format(count))\n",
    "        save_image(opic, '../results/01-autoencoder/ae/val-{}-dec.png'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how each image was encoded.  \n",
    "The next line prints the encodings for the last batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('encoded output: {}'.format(encoding.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to analyse the results inline we can grab the encoding/decoding variables and play with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(decoding[idx].cpu().data.view(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,2)\n",
    "axarr[0].imshow(x[idx].cpu().data.view(28,28))\n",
    "axarr[1].imshow(decoding[idx].cpu().data.view(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding / Decoding single images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, I am going to use the validation set again, but one can do the same with any image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Linear(in_features=64, out_features=12, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=12, out_features=3, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=12, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=128, out_features=784, bias=True)\n",
       "    (7): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1001 # select an index in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pic_data.dtype: uint8\n",
      "pic mean: 0.02145381271839142, std: 1.0174704790115356\n",
      "\n",
      "pic shape (C x H x W): torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Define the transforms to normalise the image\n",
    "pic_tt = transforms.ToTensor() # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "pic_n = transforms.Normalize((ds_mean,), (ds_std,))\n",
    "\n",
    "if torch.__version__ != '1.3.0':\n",
    "    pic_data = valid_ds.test_data[idx].numpy()\n",
    "    pic_data = pic_data[:, :, None] # (H x W x C)\n",
    "    print('pic_data.dtype: {}'.format(pic_data.dtype))\n",
    "else:\n",
    "    pic_data = valid_ds.data[idx].numpy()\n",
    "    \n",
    "pic = pic_tt(pic_data)\n",
    "pic = pic_n(pic)\n",
    "\n",
    "# print stats\n",
    "print('pic mean: {}, std: {}'.format(pic.mean(), pic.std()))\n",
    "\n",
    "# print pic shape\n",
    "print('\\npic shape (C x H x W): {}'.format(pic.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdc274052b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pic.view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model to encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded output: [[-9.222232   9.739955  -7.7938967]]\n"
     ]
    }
   ],
   "source": [
    "encoding = model.encode(pic.to(device).view(1, -1)) # note that we need to reshape the pic to be: bsx28*28 \n",
    "\n",
    "# print encoding result\n",
    "print('encoded output: {}'.format(encoding.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the decoder and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = model.decode(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdc273d99b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(decoding.cpu().data.view(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = torch.tensor([\n",
    "    [17., -10., +0.]])\n",
    "\n",
    "decoding = model.decode(encoding)\n",
    "plt.imshow(decoding.cpu().data.view(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can define a Convolutional Autoencoder, where the network is composed of a sequence of convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # b, 8, 2, 2\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.5109\n",
      "epoch [2/100], loss:0.4628\n",
      "epoch [3/100], loss:0.5199\n",
      "epoch [4/100], loss:0.4988\n",
      "epoch [5/100], loss:0.4752\n",
      "epoch [6/100], loss:0.4783\n",
      "epoch [7/100], loss:0.4565\n",
      "epoch [8/100], loss:0.4774\n",
      "epoch [9/100], loss:0.4367\n",
      "epoch [10/100], loss:0.5051\n",
      "epoch [11/100], loss:0.4323\n",
      "epoch [12/100], loss:0.4938\n",
      "epoch [13/100], loss:0.4625\n",
      "epoch [14/100], loss:0.4442\n",
      "epoch [15/100], loss:0.4359\n",
      "epoch [16/100], loss:0.4216\n",
      "epoch [17/100], loss:0.4128\n",
      "epoch [18/100], loss:0.5216\n",
      "epoch [19/100], loss:0.4163\n",
      "epoch [20/100], loss:0.4229\n",
      "epoch [21/100], loss:0.4854\n",
      "epoch [22/100], loss:0.4435\n",
      "epoch [23/100], loss:0.4405\n",
      "epoch [24/100], loss:0.3971\n",
      "epoch [25/100], loss:0.4503\n",
      "epoch [26/100], loss:0.4532\n",
      "epoch [27/100], loss:0.4384\n",
      "epoch [28/100], loss:0.4189\n",
      "epoch [29/100], loss:0.4507\n",
      "epoch [30/100], loss:0.4159\n",
      "epoch [31/100], loss:0.4487\n",
      "epoch [32/100], loss:0.4177\n",
      "epoch [33/100], loss:0.4478\n",
      "epoch [34/100], loss:0.4493\n",
      "epoch [35/100], loss:0.4836\n",
      "epoch [36/100], loss:0.4739\n",
      "epoch [37/100], loss:0.4223\n",
      "epoch [38/100], loss:0.4179\n",
      "epoch [39/100], loss:0.4717\n",
      "epoch [40/100], loss:0.4035\n",
      "epoch [41/100], loss:0.4273\n",
      "epoch [42/100], loss:0.4396\n",
      "epoch [43/100], loss:0.4240\n",
      "epoch [44/100], loss:0.4513\n",
      "epoch [45/100], loss:0.4021\n",
      "epoch [46/100], loss:0.4341\n",
      "epoch [47/100], loss:0.3915\n",
      "epoch [48/100], loss:0.4056\n",
      "epoch [49/100], loss:0.4917\n",
      "epoch [50/100], loss:0.4463\n",
      "epoch [51/100], loss:0.4041\n",
      "epoch [52/100], loss:0.4991\n",
      "epoch [53/100], loss:0.4324\n",
      "epoch [54/100], loss:0.4214\n",
      "epoch [55/100], loss:0.4178\n",
      "epoch [56/100], loss:0.4286\n",
      "epoch [57/100], loss:0.4134\n",
      "epoch [58/100], loss:0.4234\n",
      "epoch [59/100], loss:0.4489\n",
      "epoch [60/100], loss:0.4033\n",
      "epoch [61/100], loss:0.4821\n",
      "epoch [62/100], loss:0.4093\n",
      "epoch [63/100], loss:0.4131\n",
      "epoch [64/100], loss:0.4737\n",
      "epoch [65/100], loss:0.4279\n",
      "epoch [66/100], loss:0.4410\n",
      "epoch [67/100], loss:0.4481\n",
      "epoch [68/100], loss:0.3976\n",
      "epoch [69/100], loss:0.4504\n",
      "epoch [70/100], loss:0.3888\n",
      "epoch [71/100], loss:0.3833\n",
      "epoch [72/100], loss:0.4279\n",
      "epoch [73/100], loss:0.3964\n",
      "epoch [74/100], loss:0.3978\n",
      "epoch [75/100], loss:0.4131\n",
      "epoch [76/100], loss:0.4012\n",
      "epoch [77/100], loss:0.4147\n",
      "epoch [78/100], loss:0.4499\n",
      "epoch [79/100], loss:0.3835\n",
      "epoch [80/100], loss:0.4146\n",
      "epoch [81/100], loss:0.4270\n",
      "epoch [82/100], loss:0.3776\n",
      "epoch [83/100], loss:0.4476\n",
      "epoch [84/100], loss:0.4090\n",
      "epoch [85/100], loss:0.4182\n",
      "epoch [86/100], loss:0.4433\n",
      "epoch [87/100], loss:0.4532\n",
      "epoch [88/100], loss:0.4058\n",
      "epoch [89/100], loss:0.4659\n",
      "epoch [90/100], loss:0.4265\n",
      "epoch [91/100], loss:0.4224\n",
      "epoch [92/100], loss:0.4296\n",
      "epoch [93/100], loss:0.4599\n",
      "epoch [94/100], loss:0.4055\n",
      "epoch [95/100], loss:0.4219\n",
      "epoch [96/100], loss:0.4640\n",
      "epoch [97/100], loss:0.4497\n",
      "epoch [98/100], loss:0.4238\n",
      "epoch [99/100], loss:0.4529\n",
      "epoch [100/100], loss:0.3765\n"
     ]
    }
   ],
   "source": [
    "model = ConvAutoencoder()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "if device == 'cuda': model.cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in train_dl:\n",
    "        x = x.to(device)  # resize to be a vector bsx28*28\n",
    "\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.data))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        ipic = toImg(x.cpu().data)\n",
    "        opic = toImg(output.cpu().data)\n",
    "\n",
    "        save_image(opic, '../results/01-autoencoder/convae/image_{}_conv_o.png'.format(epoch))\n",
    "        save_image(ipic, '../results/01-autoencoder/convae/image_{}_conv_i.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved as: ./saved-mdls/01-autoencoder/convautoencoder_100e.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ConvAutoencoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# As suggested in https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "filename = '../saved-mdls/01-autoencoder/convautoencoder_{}e.pt'.format(epoch+1)\n",
    "torch.save(model, filename)\n",
    "\n",
    "print('model saved as: {}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): ConvTranspose2d(16, 8, kernel_size=(5, 5), stride=(3, 3), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): ConvTranspose2d(8, 1, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1001 # select an index in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pic_data.dtype: uint8\n",
      "pic mean: 0.02145381271839142, std: 1.0174704790115356\n",
      "\n",
      "pic shape (C x H x W): torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Define the transforms to normalise the image\n",
    "pic_tt = transforms.ToTensor() # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "pic_n = transforms.Normalize((ds_mean,), (ds_std,))\n",
    "\n",
    "if torch.__version__ != '1.3.0':\n",
    "    pic_data = valid_ds.test_data[idx].numpy()\n",
    "    pic_data = pic_data[:, :, None] # (H x W x C)\n",
    "    print('pic_data.dtype: {}'.format(pic_data.dtype))\n",
    "else:\n",
    "    pic_data = valid_ds.data[idx].numpy()\n",
    "    \n",
    "pic = pic_tt(pic_data)\n",
    "pic = pic_n(pic)\n",
    "\n",
    "# print stats\n",
    "print('pic mean: {}, std: {}'.format(pic.mean(), pic.std()))\n",
    "\n",
    "# print pic shape\n",
    "print('\\npic shape (C x H x W): {}'.format(pic.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdc2602a2b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pic.view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model to encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(pic.to(device)[None, :, :, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded output: [[[[10.795247    6.077096  ]\n",
      "   [ 5.69029     0.        ]]\n",
      "\n",
      "  [[ 2.3519301   2.3519301 ]\n",
      "   [ 2.3519301   2.3519301 ]]\n",
      "\n",
      "  [[ 6.9652967   6.9652967 ]\n",
      "   [ 0.5121887   0.5121887 ]]\n",
      "\n",
      "  [[ 1.8640186   7.8633065 ]\n",
      "   [ 5.015077    9.722703  ]]\n",
      "\n",
      "  [[ 0.46509135  0.46509135]\n",
      "   [ 8.607014    8.607014  ]]\n",
      "\n",
      "  [[ 9.282421    5.930196  ]\n",
      "   [ 4.88288     1.791863  ]]\n",
      "\n",
      "  [[ 0.          7.888112  ]\n",
      "   [ 1.9434392   6.8836126 ]]\n",
      "\n",
      "  [[12.865752   12.865752  ]\n",
      "   [12.865752   12.865752  ]]]]\n"
     ]
    }
   ],
   "source": [
    "encoding = model.encode(pic.to(device)[None, :, :, :]) # note that we need to reshape the pic to be: bsx28*28 \n",
    "\n",
    "# print encoding result\n",
    "print('encoded output: {}'.format(encoding.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the decoder and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = model.decode(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdc261bfd68>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(decoding.cpu().data.view(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discussed the problem of dimentionality reduction, and used encoder-decoder architectures to address it.\n",
    "\n",
    "At this point, one question might arise: what is the link between autoencoders and content generation?\n",
    "\n",
    "we could be tempted to think that, if the latent space is regular enough (well “organized” by the encoder during the training process), we could take a point randomly from that latent space and decode it to get a new content. The decoder would then act more or less like the generator of a Generative Adversarial Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done before, we can generate new data by decoding points that are randomly sampled from the latent space. The quality and relevance of generated data depend on the regularity of the latent space.\n",
    "\n",
    "The problem is that the regularity of the latent space for autoencoders depends on the distribution of the data in the initial space, the dimension of the latent space and the architecture of the encoder. \n",
    "It is difficult to ensure that the encoder will organize the latent space in a smart way compatible with the generative process we just described.\n",
    "\n",
    "\n",
    "This lack of structure among the encoded data into the latent space is normal since during training we never enforced such organization of the latern space.\n",
    "We only trained to encode and decode so that we can minimize the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the decoder of our autoencoder for generative purpose, we have to be sure that the latent space is regular enough. One possible solution to obtain such regularity is to introduce explicit regularisation during the training process. \n",
    "\n",
    "A variational autoencoder is an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a standard autoencoder, a variational autoencoder is an architecture composed of both an encoder and a decoder and that is trained to minimise the reconstruction error between the encoded-decoded data and the initial data. However, in order to introduce some regularisation of the latent space, we proceed to a slight modification of the encoding-decoding process: instead of encoding an input as a single point, we encode it as a distribution over the latent space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, the loss function that is minimised when training a VAE is composed of a “reconstruction term” (on the final layer), that tends to make the encoding-decoding scheme as performant as possible, and a “regularisation term” (on the latent layer), that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to measure the difference between probability distribution is using the Kullback-Leibler divergence.\n",
    "\n",
    "In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. \n",
    "The reason why an input is encoded as a distribution with some variance instead of a single point is that it makes possible to express very naturally the latent space regularisation: the distributions returned by the encoder are enforced to be close to a standard normal distribution. \n",
    "Moreover, the Kullback-Leibler divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularity that is expected from the latent space in order to make generative process possible can be expressed through two main properties: \n",
    "- continuity (two close points in the latent space should not give two completely different contents once decoded)\n",
    "- completeness (for a chosen distribution, a point sampled from the latent space should give “meaningful” content once decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A point of the latent space that would be halfway between the means of two encoded distributions coming from different training data should be decoded in something that is somewhere between the data that gave the first distribution and the data that gave the second distribution as it may be sampled by the autoencoder in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational autoencoders encode inputs as distributions (e.g. mean and std) and not as simple points (or vectors). The regularisation of the latent space is obtained constraining the distribution to be as close as possible to a Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$ : data  \n",
    "$z$ : encoded rapresentation of $x$\n",
    "\n",
    "We also assume that $z$ cannot be directly observed. Hewover, the latent representation $z$ can be sampled from the prior distribution $p(z)$\n",
    "\n",
    "The data $x$ can be sampled from the conditional likelihood distribution: $p(x|z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this framework, we can consider the probabilistic versions of encoders and decoders (instead of the deterministic ones that we described before).\n",
    "\n",
    "- The “probabilistic decoder” is defined by $p(x|z)$, that describes the distribution of the decoded variable given the encoded one.\n",
    "\n",
    "- The “probabilistic encoder” is defined by $p(z|x)$, that describes the distribution of the encoded variable given the decoded one.\n",
    "\n",
    "Where of course, the link between the two is obtained through the Bayes' theorem: $p(z|x) = p(x|z)p(z)/p(x)$.\n",
    "\n",
    "The previous equation also means that the encoder $p(z|x)$ depends on the prior $p(z)$, or on the encoded representation of $z$ in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume:\n",
    "\n",
    "$p(z) = \\mathcal{N}(0, I)$  \n",
    "$p(x|z) = \\mathcal{N}(f(z), cI)$, where $f \\in F$ (family of functions), and $I$ is the identity matrix, $c$ constant > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider $f$ to be well defined, fixed (and known), and if we know $p(x)$, can have all the elements to solve this problem and calculate $p(z|x)$. However calculating $p(x)$ is often intractable as $p(x)=\\int p(x|u)p(u)du$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational inference (VI) can be used to approximate complex distributions and hence we can use it to approximate $p(z|x)$.\n",
    "\n",
    "The idea is to set a parametrised family of distribution (for example the family of Gaussians, whose parameters are the mean and the covariance) and to look for the best approximation of the target distribution among this family.\n",
    "\n",
    "The best element in the family is one that minimise a given approximation error measurement (most of the time the Kullback-Leibler divergence between approximation and target).\n",
    "\n",
    "This can be done using gradient descent over the parameters that describe the family [REF]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we assume $p(z|x)$, our target distribution as a Gaussian:\n",
    "\n",
    "$p(z|x) = \\mathcal{N}(g(x), h(x))$,\n",
    "\n",
    "This means that we have chosen a Gaussian to approximate $p(z|x)$, whose mean and covariance are defined by two functions, $g$ and $h$, of the parameter $x$ (the data).\n",
    "\n",
    "The problem than become that of finding the best approximation among this family by optimising the functions g and h (or better, their parameters) to minimise the Kullback-Leibler (KL) divergence between the approximation $\\mathcal{N}(g(x), h(x))$ and the target $p(z|x)$, or:\n",
    "\n",
    "\\begin{equation*}\n",
    "(g^*, h^*) = arg \\min_{g,h} KL(\\mathcal{N}(g(x), h(x)), p(z|x)).\n",
    "\\label{eq:optimal-encoder} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "This would give us the optimal encoder $p^*(z|x) = \\mathcal{N}(g^*(x), h^*(x))$.\n",
    "\n",
    "Eq. (1), with a little bit of work (see [Understanding Variational Autoencoders](\n",
    "https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)) can be written as:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "(g^*, h^*) = \\underset{g,h\\in (GxH)}{\\operatorname{argmax}} \\{\n",
    "\\mathbb{E}_{z \\sim \\mathcal{N}(g(x), h(x))} [  log(p(x|z) ] - KL( \\mathcal{N}(g(x), h(x)), p(z)) \\}\n",
    "\\label{eq:optimal-encoder-2} \\tag{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "All this reasoning was based on the assumption that we know $f(z)$. In fact, given that we know $f(z)$, we can calculate $p(x|z)$ (the decoder) and then we calculate the encoder $p(z|x)$ approximating it via variational inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in practise, $f(z)$ is not known and need to be chosen. In fact, the objective is to find a couple encoder-decoder with a regularised latent space, and the decoder $p(x|z)$ is a function of the function $f(z)$.\n",
    "\n",
    "We have also realised that the encoder $p(z|x)$ depends on the prior $p(z)$ (Bayes' theorem).\n",
    "\n",
    "As $p(z|x)$ can be approximated (by variational inference) from $p(z)$, and given that $p(x|z)$ and $p(z)$ have been set to be simple standard Gaussian, \n",
    "we only have two parameters in our model: $c$ (that defines the variance of the posterior, $p(x|z)$) and the function $f$ (that defines the mean of the posterior).\n",
    "\n",
    "In short:\n",
    "- the encoder-decoder performance depends on the function $f(z)$\n",
    "- the regularity of the latent space depends on the prior $p(z)$\n",
    "\n",
    "This makes sense intuitively, as $f(z)$ defines the encoder, and $p(z)$ defines the latent space distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different functions $f(z)$ define different encoders.\n",
    "\n",
    "We would like to find the best $f$ that maximise the encoding/decoding performance, or:\n",
    "\n",
    "We want to choose the function $f$ that maximises the expected log-likelihood of $x$ (the data) given $z$ (its latent representation or encoding) when $z$ is sampled from $p^*(z|x)$. Where $p^*(z|x)$ is the optimal encoder, calculated as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given input $x$, we want to:\n",
    "- maximise the probability to have $\\hat x = x$\n",
    "- when we sample $z$ from the distribution $p^*(z|x)$ (encoding)\n",
    "- and then sample $\\hat x$ from the distribution $p(x|z)$ (decoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write this as:\n",
    "\n",
    "\\begin{equation}\n",
    "f^* = \\underset{f\\in F}{\\operatorname{argmax}}\n",
    "\\mathbb{E}_{z \\sim p^*(z|x)} [ \\log p(x|z) ] \n",
    "\\label{eq:optimal-decoder} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Note that, if $p(x|z) = \\mathcal{N}(f(z), cI)$, then\n",
    "$\\log p(x|z) = - \\frac{||x-f(z)||^2}{2c}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder/Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now put everything together (Eq.(1), (2), (3)):\n",
    "\n",
    "\\begin{equation*}\n",
    "(f^*, g^*, h^*) = \\underset{f,g,h\\in (FxGxH)}{\\operatorname{argmax}} \\{\n",
    "\\mathbb{E}_{z \\sim \\mathcal{N}(g(x), h(x))} [  log(p(x|z) ] -  KL( \\mathcal{N}(g(x), h(x)), p(z)) \\}\n",
    "\\label{eq:optimal-encoder-2}\n",
    "\\end{equation*}\n",
    "\n",
    "$\\tag{4}$\n",
    "\n",
    "with $\\log p(x|z) = - \\frac{||x-f(z)||^2}{2c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comments:*\n",
    "\n",
    "The previous Eq (4), includes:\n",
    "- the reconstruction error between $x$ and $f(z)$ (objective 1)\n",
    "- the regularisation term given by the $KL$ divergence between $\\mathcal{N}(g(x), h(x))$ and $p(z)$ (which is a standard Gaussian) (objective 2). \n",
    "\n",
    "The constant $c$ acts as the balance between these two objectives: \n",
    "\n",
    "The higher $c$ is, the more we assume a high variance around $f(z)$ for the probabilistic decoder in our model and, the more we favour the regularisation term over the reconstruction term.  \n",
    "The opposite stands if $c$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a variational encoder with neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can’t easily optimise over the entire space of functions.\n",
    "One way to solve this problem is to constrain the optimisation domain and decide to express $f$, $g$ and $h$ as neural networks.  \n",
    "In this case, $F$, $G$ and $H$ correspond respectively to the families of functions defined by the networks architectures and the optimisation is done over the parameters of these networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise, $g$ and $h$, which together define the encoder $p(z|x)$ are defined to share the initial part of their network architecture.  \n",
    "\n",
    "![encoder-part-of-the-VAE](pics/encoder-part-of-the-VAE.png)\n",
    "**Figure: Encoder part of the VAE (from [Understanding Variational Autoencoders](\n",
    "https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73))**\n",
    "\n",
    "\n",
    "Moreover, to simplify the problem, $\\mathcal{N}(g(x), h(x))$ is assumed as a multidimensional Gaussian distribution with diagonal covariance matrix.\n",
    "\n",
    "Under this assumption, the function $h(x)$ which defines the covariance matrix, is the vector of the diagonal elements.\n",
    "\n",
    "To summarise, the output of the encoder part is a Gaussian with both mean $g$ and covariance $h$ that are functions of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder network $f$ needs instead to model $p(x|z)$ that we defined as a Gaussian with unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the overall architecture is then obtained by concatenating the encoder and the decoder parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparametrisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last element that remains is related to the fact that we need to sample from the encoded distribution $p(z|x)$ in a way that we can backpropagate the error through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reparametrisation allows us to do exactly that: makes the gradient descent possible despite the random sampling that occurs halfw aythrough the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real trick is to understand that $z$ is a random variable sampled from a Gaussian distribution with mean $g(x)$ and variance $h(x)$.\n",
    "\n",
    "This means that we can express $z$ as:\n",
    "\n",
    "$z=h(x) \\eta + g(x)$,\n",
    "\n",
    "with $\\eta \\sim \\mathcal{N}(0, I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we have sandboxed the sampling to only happen on the variable $\\eta$, while we can do gradient descent over $h(x)$ and $g(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the _objective function_ of the variational autoencoder is defined by Eq. (4), where the expectation operator is replaced, most of the time by a Monte-Carlo approximation (sometimes obtained as a single draw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final architecture is sketched out in the next figure (from [Understanding Variational Autoencoders](\n",
    "https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![variational-autoencoder.png](pics/variational-autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure: architecture of a variational autoencoder (C = $\\frac{1}{2c}$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "ds_mean = 0.1307\n",
    "ds_std = 0.3081\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((ds_mean,), (ds_std,)) # The first tuple (0.5, 0.5, 0.5) is the mean for all three channels and the second (0.5, 0.5, 0.5) is the standard deviation for all three channels.\n",
    "])\n",
    "\n",
    "### Grab the dataset\n",
    "\n",
    "train_ds = MNIST('./data', train=True, transform=img_transform, download=True)\n",
    "valid_ds = MNIST('./data', train=False, transform=img_transform, download=True)\n",
    "\n",
    "# plt.imshow(train_ds.data[1])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.base_encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 400),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "            \n",
    "        self.mean_encoder = nn.Sequential(\n",
    "            self.base_encoder,\n",
    "            nn.Linear(400, 20)\n",
    "        )\n",
    "        \n",
    "        self.var_encoder = nn.Sequential(\n",
    "            self.base_encoder,\n",
    "            nn.Linear(400, 20)\n",
    "        )\n",
    "                \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(20, 400),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(400, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "                \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.mean_encoder(x), self.var_encoder(x)        \n",
    "        z = self.reparametrize(mu, logvar)        \n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.mean_encoder(x), self.var_encoder(x)        \n",
    "    \n",
    "    def decode(self, x):\n",
    "        mu, logvar = x[0], x[1]\n",
    "        z = self.reparametrize(mu, logvar)        \n",
    "        return self.decoder(z), mu, logvar\n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        \"\"\"This function implements the reparametrisation trick:\n",
    "        z = h(x)*eta+g(x), eta is Gaussian\n",
    "        \"\"\"\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "                    \n",
    "        # sample the normal distribution        \n",
    "        if torch.cuda.is_available():\n",
    "            eta = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eta = torch.FloatTensor(std.size()).normal_()                \n",
    "            \n",
    "        return eta.mul(std).add_(mu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (base_encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "  )\n",
       "  (mean_encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (1): Linear(in_features=400, out_features=20, bias=True)\n",
       "  )\n",
       "  (var_encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (1): Linear(in_features=400, out_features=20, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=400, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=400, out_features=784, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss(size_average=False)\n",
    "bce = nn.BCELoss(size_average=False)\n",
    "\n",
    "\n",
    "def loss_f(x_hat, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    x_hat: generated img.\n",
    "    x: origin img.\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance (log(sigma^2))\n",
    "    \"\"\"\n",
    "    \n",
    "    # E[log P(x|z)]     \n",
    "    approx_err = bce(x_hat, x)  # (-||x-f(z)||^2/(2c))\n",
    "    \n",
    "    # The divergence between two Gaussians can be calculated in closed form.\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    _kl = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KL = torch.sum(_kl).mul_(-0.5)\n",
    "            \n",
    "    return approx_err + KL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 548.798096\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: -10400.995117\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: -13520.972656\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: -15273.527344\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: -15952.123047\n",
      ">>  Epoch: 0 Average loss: -12749.0210\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: -16029.354492\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: -16067.035156\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: -16273.087891\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: -16572.113281\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: -16374.041016\n",
      ">>  Epoch: 1 Average loss: -16285.1504\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: -16515.455078\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: -16544.494141\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: -16339.613281\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: -16133.729492\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: -16039.532227\n",
      ">>  Epoch: 2 Average loss: -16280.5579\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: -16026.655273\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: -16169.336914\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: -16350.911133\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: -16016.779297\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: -16301.458008\n",
      ">>  Epoch: 3 Average loss: -16044.4319\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: -16284.871094\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: -16331.748047\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: -15999.494141\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: -15769.664062\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: -15759.160156\n",
      ">>  Epoch: 4 Average loss: -15910.8688\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: -16160.987305\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: -16652.466797\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: -15019.367188\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: -15418.454102\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: -15785.478516\n",
      ">>  Epoch: 5 Average loss: -15739.4408\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: -15510.125977\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: -15574.984375\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: -15398.647461\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: -15153.815430\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: -15735.817383\n",
      ">>  Epoch: 6 Average loss: -15629.2851\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: -15564.513672\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: -15361.147461\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: -15257.525391\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: -15549.546875\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: -15512.875977\n",
      ">>  Epoch: 7 Average loss: -15618.3374\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: -15687.753906\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: -15689.877930\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: -15374.916016\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: -15942.115234\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: -14915.119141\n",
      ">>  Epoch: 8 Average loss: -15655.1822\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: -14978.934570\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: -15950.153320\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: -16483.396484\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: -15933.653320\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: -16408.232422\n",
      ">>  Epoch: 9 Average loss: -15699.3873\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: -15647.618164\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: -16330.379883\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: -15949.466797\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: -16028.821289\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: -15954.647461\n",
      ">>  Epoch: 10 Average loss: -15707.1465\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: -15906.910156\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: -16228.386719\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: -15131.728516\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: -15891.877930\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: -16029.520508\n",
      ">>  Epoch: 11 Average loss: -15761.8662\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: -15814.916016\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: -15640.537109\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: -15681.725586\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: -15612.296875\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: -15969.593750\n",
      ">>  Epoch: 12 Average loss: -15755.1473\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: -15734.547852\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: -15912.906250\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: -15386.783203\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: -15979.217773\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: -15417.867188\n",
      ">>  Epoch: 13 Average loss: -15720.0671\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: -15656.513672\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: -16088.073242\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: -15439.639648\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: -15694.291992\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: -15735.992188\n",
      ">>  Epoch: 14 Average loss: -15731.8497\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: -16031.658203\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: -15363.505859\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: -15335.965820\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: -15611.947266\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: -15649.885742\n",
      ">>  Epoch: 15 Average loss: -15741.5805\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: -15287.258789\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: -15810.520508\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: -15941.052734\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: -15806.700195\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: -16056.616211\n",
      ">>  Epoch: 16 Average loss: -15741.6051\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: -15775.357422\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: -15751.205078\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: -16333.038086\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: -16151.360352\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: -16332.794922\n",
      ">>  Epoch: 17 Average loss: -15736.9253\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: -15776.823242\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: -15912.367188\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: -15632.143555\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: -15429.823242\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: -16084.385742\n",
      ">>  Epoch: 18 Average loss: -15756.3129\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: -15728.345703\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: -15664.066406\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: -15685.458984\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: -15815.618164\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: -14626.311523\n",
      ">>  Epoch: 19 Average loss: -15748.6681\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: -15371.667969\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: -15351.987305\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: -15516.359375\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: -15894.355469\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: -15774.041016\n",
      ">>  Epoch: 20 Average loss: -15764.9767\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: -16372.184570\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: -15606.973633\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: -15101.914062\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: -15878.699219\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: -15291.368164\n",
      ">>  Epoch: 21 Average loss: -15746.8408\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: -15855.291992\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: -16354.753906\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: -16244.956055\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: -16145.729492\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: -15999.291992\n",
      ">>  Epoch: 22 Average loss: -15754.9529\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: -15501.169922\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: -15767.891602\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: -15450.406250\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: -16731.027344\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: -15620.701172\n",
      ">>  Epoch: 23 Average loss: -15784.9061\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: -15639.106445\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: -16171.797852\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: -16093.580078\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: -15425.541992\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: -15944.828125\n",
      ">>  Epoch: 24 Average loss: -15775.7069\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: -15554.327148\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: -15968.566406\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: -15659.537109\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: -15878.447266\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: -16219.924805\n",
      ">>  Epoch: 25 Average loss: -15797.3291\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: -15969.476562\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: -15485.171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: -15726.582031\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: -15938.704102\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: -15694.555664\n",
      ">>  Epoch: 26 Average loss: -15808.7790\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: -15663.542969\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: -15626.659180\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: -16402.958984\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: -15684.948242\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: -15679.839844\n",
      ">>  Epoch: 27 Average loss: -15828.3127\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: -15980.693359\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: -15336.357422\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: -15683.900391\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: -15997.925781\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: -16086.597656\n",
      ">>  Epoch: 28 Average loss: -15855.4075\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: -15888.863281\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: -15684.571289\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: -15961.804688\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: -15849.370117\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: -15698.311523\n",
      ">>  Epoch: 29 Average loss: -15878.0026\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: -16085.376953\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: -16326.175781\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: -15949.460938\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: -16597.437500\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: -16148.072266\n",
      ">>  Epoch: 30 Average loss: -15926.7902\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: -15720.159180\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: -15995.453125\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: -16108.129883\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: -16219.212891\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: -15874.607422\n",
      ">>  Epoch: 31 Average loss: -15964.4744\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: -15681.047852\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: -15873.328125\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: -15883.899414\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: -16537.060547\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: -15421.623047\n",
      ">>  Epoch: 32 Average loss: -16004.1485\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: -16307.760742\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: -16323.730469\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: -15635.327148\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: -15912.753906\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: -16238.466797\n",
      ">>  Epoch: 33 Average loss: -16063.7850\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: -16172.517578\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: -15760.675781\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: -16394.808594\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: -16498.275391\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: -15712.812500\n",
      ">>  Epoch: 34 Average loss: -16138.8057\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: -16025.337891\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: -16540.029297\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: -15970.066406\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: -15916.967773\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: -16363.433594\n",
      ">>  Epoch: 35 Average loss: -16232.1018\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: -15838.261719\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: -15826.887695\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: -16310.962891\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: -16837.082031\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: -16399.076172\n",
      ">>  Epoch: 36 Average loss: -16367.7808\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: -15912.750000\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: -16463.654297\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: -16507.283203\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: -16087.551758\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: -16615.046875\n",
      ">>  Epoch: 37 Average loss: -16458.7341\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: -17020.490234\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: -16587.585938\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: -16319.344727\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: -16382.368164\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: -16504.347656\n",
      ">>  Epoch: 38 Average loss: -16632.4627\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: -16381.448242\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: -16697.320312\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: -16804.128906\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: -16615.097656\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: -16981.410156\n",
      ">>  Epoch: 39 Average loss: -16810.5873\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: -16861.273438\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: -17466.968750\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: -17402.136719\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: -16999.201172\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: -17495.255859\n",
      ">>  Epoch: 40 Average loss: -17060.1137\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: -17004.109375\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: -17359.103516\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: -17511.921875\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: -17389.585938\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: -17074.890625\n",
      ">>  Epoch: 41 Average loss: -17282.8759\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: -17634.613281\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: -17447.382812\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: -17465.964844\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: -17410.089844\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: -17276.812500\n",
      ">>  Epoch: 42 Average loss: -17441.8183\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: -17767.224609\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: -17329.523438\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: -17272.613281\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: -17227.455078\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: -17635.570312\n",
      ">>  Epoch: 43 Average loss: -17500.9153\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: -17490.695312\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: -17791.738281\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: -17543.478516\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: -17350.167969\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: -17349.462891\n",
      ">>  Epoch: 44 Average loss: -17593.8134\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: -17580.316406\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: -17385.085938\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: -17382.095703\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: -17429.287109\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: -17536.187500\n",
      ">>  Epoch: 45 Average loss: -17564.7666\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: -17598.343750\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: -17388.191406\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: -17293.800781\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: -17492.017578\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: -17604.408203\n",
      ">>  Epoch: 46 Average loss: -17465.0792\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: -17610.906250\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: -17191.480469\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: -17691.476562\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: -17332.033203\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: -17568.968750\n",
      ">>  Epoch: 47 Average loss: -17531.3173\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: -17307.283203\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: -17492.416016\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: -17337.798828\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: -17336.730469\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: -17419.738281\n",
      ">>  Epoch: 48 Average loss: -17327.9903\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: -17431.869141\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: -17365.650391\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: -17182.134766\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: -16896.330078\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: -17013.939453\n",
      ">>  Epoch: 49 Average loss: -17136.9201\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: -17015.525391\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: -16782.992188\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: -17026.062500\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: -17217.783203\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: -17103.363281\n",
      ">>  Epoch: 50 Average loss: -17111.1771\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: -17389.720703\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tLoss: -17199.408203\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: -17102.441406\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: -17427.525391\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: -16863.777344\n",
      ">>  Epoch: 51 Average loss: -17124.5835\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: -16963.330078\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tLoss: -16904.535156\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: -17380.410156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: -16948.828125\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: -17177.294922\n",
      ">>  Epoch: 52 Average loss: -17001.3476\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: -16871.671875\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tLoss: -16856.982422\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: -16822.964844\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tLoss: -16449.992188\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: -17396.875000\n",
      ">>  Epoch: 53 Average loss: -16953.9299\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: -17157.412109\n",
      "Train Epoch: 54 [12800/60000 (21%)]\tLoss: -16841.775391\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tLoss: -16886.615234\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tLoss: -16942.320312\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: -16818.152344\n",
      ">>  Epoch: 54 Average loss: -16996.4448\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: -16912.093750\n",
      "Train Epoch: 55 [12800/60000 (21%)]\tLoss: -16525.378906\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tLoss: -17093.421875\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tLoss: -17006.072266\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: -16766.722656\n",
      ">>  Epoch: 55 Average loss: -16809.1004\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: -16664.152344\n",
      "Train Epoch: 56 [12800/60000 (21%)]\tLoss: -16771.865234\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tLoss: -16739.525391\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tLoss: -16652.421875\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: -17241.503906\n",
      ">>  Epoch: 56 Average loss: -16875.5294\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: -16731.355469\n",
      "Train Epoch: 57 [12800/60000 (21%)]\tLoss: -16529.789062\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tLoss: -16789.970703\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tLoss: -16686.048828\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: -17631.421875\n",
      ">>  Epoch: 57 Average loss: -16833.4013\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: -16920.191406\n",
      "Train Epoch: 58 [12800/60000 (21%)]\tLoss: -17039.376953\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tLoss: -16618.492188\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tLoss: -16558.892578\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: -16741.796875\n",
      ">>  Epoch: 58 Average loss: -16736.3788\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: -16661.085938\n",
      "Train Epoch: 59 [12800/60000 (21%)]\tLoss: -16362.338867\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tLoss: -16555.357422\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tLoss: -16383.298828\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: -16890.437500\n",
      ">>  Epoch: 59 Average loss: -16539.4592\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: -16634.007812\n",
      "Train Epoch: 60 [12800/60000 (21%)]\tLoss: -16557.408203\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tLoss: -16968.511719\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tLoss: -16812.199219\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tLoss: -16647.976562\n",
      ">>  Epoch: 60 Average loss: -16661.2688\n",
      "Train Epoch: 61 [0/60000 (0%)]\tLoss: -17091.906250\n",
      "Train Epoch: 61 [12800/60000 (21%)]\tLoss: -16241.574219\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tLoss: -16217.345703\n",
      "Train Epoch: 61 [38400/60000 (64%)]\tLoss: -16891.384766\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tLoss: -16775.207031\n",
      ">>  Epoch: 61 Average loss: -16609.7100\n",
      "Train Epoch: 62 [0/60000 (0%)]\tLoss: -17134.527344\n",
      "Train Epoch: 62 [12800/60000 (21%)]\tLoss: -16760.613281\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tLoss: -16567.193359\n",
      "Train Epoch: 62 [38400/60000 (64%)]\tLoss: -16474.320312\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tLoss: -16455.552734\n",
      ">>  Epoch: 62 Average loss: -16746.5140\n",
      "Train Epoch: 63 [0/60000 (0%)]\tLoss: -17373.203125\n",
      "Train Epoch: 63 [12800/60000 (21%)]\tLoss: -16973.087891\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tLoss: -16784.939453\n",
      "Train Epoch: 63 [38400/60000 (64%)]\tLoss: -17112.052734\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tLoss: -16774.017578\n",
      ">>  Epoch: 63 Average loss: -16980.7624\n",
      "Train Epoch: 64 [0/60000 (0%)]\tLoss: -17215.111328\n",
      "Train Epoch: 64 [12800/60000 (21%)]\tLoss: -16998.052734\n",
      "Train Epoch: 64 [25600/60000 (43%)]\tLoss: -16724.621094\n",
      "Train Epoch: 64 [38400/60000 (64%)]\tLoss: -16849.472656\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tLoss: -16990.341797\n",
      ">>  Epoch: 64 Average loss: -16977.7651\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: -16878.310547\n",
      "Train Epoch: 65 [12800/60000 (21%)]\tLoss: -16966.154297\n",
      "Train Epoch: 65 [25600/60000 (43%)]\tLoss: -16739.839844\n",
      "Train Epoch: 65 [38400/60000 (64%)]\tLoss: -16463.107422\n",
      "Train Epoch: 65 [51200/60000 (85%)]\tLoss: -17488.527344\n",
      ">>  Epoch: 65 Average loss: -16794.3002\n",
      "Train Epoch: 66 [0/60000 (0%)]\tLoss: -16805.675781\n",
      "Train Epoch: 66 [12800/60000 (21%)]\tLoss: -16661.839844\n",
      "Train Epoch: 66 [25600/60000 (43%)]\tLoss: -16651.310547\n",
      "Train Epoch: 66 [38400/60000 (64%)]\tLoss: -17002.986328\n",
      "Train Epoch: 66 [51200/60000 (85%)]\tLoss: -17010.789062\n",
      ">>  Epoch: 66 Average loss: -16781.7397\n",
      "Train Epoch: 67 [0/60000 (0%)]\tLoss: -17037.335938\n",
      "Train Epoch: 67 [12800/60000 (21%)]\tLoss: -16572.474609\n",
      "Train Epoch: 67 [25600/60000 (43%)]\tLoss: -16418.179688\n",
      "Train Epoch: 67 [38400/60000 (64%)]\tLoss: -17102.197266\n",
      "Train Epoch: 67 [51200/60000 (85%)]\tLoss: -16854.257812\n",
      ">>  Epoch: 67 Average loss: -16788.2711\n",
      "Train Epoch: 68 [0/60000 (0%)]\tLoss: -16557.568359\n",
      "Train Epoch: 68 [12800/60000 (21%)]\tLoss: -16540.048828\n",
      "Train Epoch: 68 [25600/60000 (43%)]\tLoss: -16928.925781\n",
      "Train Epoch: 68 [38400/60000 (64%)]\tLoss: -16511.935547\n",
      "Train Epoch: 68 [51200/60000 (85%)]\tLoss: -16502.919922\n",
      ">>  Epoch: 68 Average loss: -16653.5732\n",
      "Train Epoch: 69 [0/60000 (0%)]\tLoss: -17173.617188\n",
      "Train Epoch: 69 [12800/60000 (21%)]\tLoss: -16657.203125\n",
      "Train Epoch: 69 [25600/60000 (43%)]\tLoss: -16737.843750\n",
      "Train Epoch: 69 [38400/60000 (64%)]\tLoss: -16758.394531\n",
      "Train Epoch: 69 [51200/60000 (85%)]\tLoss: -16613.031250\n",
      ">>  Epoch: 69 Average loss: -16693.8696\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: -16670.583984\n",
      "Train Epoch: 70 [12800/60000 (21%)]\tLoss: -16123.583984\n",
      "Train Epoch: 70 [25600/60000 (43%)]\tLoss: -17051.111328\n",
      "Train Epoch: 70 [38400/60000 (64%)]\tLoss: -16375.357422\n",
      "Train Epoch: 70 [51200/60000 (85%)]\tLoss: -16055.228516\n",
      ">>  Epoch: 70 Average loss: -16574.8070\n",
      "Train Epoch: 71 [0/60000 (0%)]\tLoss: -17300.732422\n",
      "Train Epoch: 71 [12800/60000 (21%)]\tLoss: -16737.800781\n",
      "Train Epoch: 71 [25600/60000 (43%)]\tLoss: -16490.570312\n",
      "Train Epoch: 71 [38400/60000 (64%)]\tLoss: -16617.035156\n",
      "Train Epoch: 71 [51200/60000 (85%)]\tLoss: -17057.218750\n",
      ">>  Epoch: 71 Average loss: -16812.1958\n",
      "Train Epoch: 72 [0/60000 (0%)]\tLoss: -16844.027344\n",
      "Train Epoch: 72 [12800/60000 (21%)]\tLoss: -17196.638672\n",
      "Train Epoch: 72 [25600/60000 (43%)]\tLoss: -17155.652344\n",
      "Train Epoch: 72 [38400/60000 (64%)]\tLoss: -16815.871094\n",
      "Train Epoch: 72 [51200/60000 (85%)]\tLoss: -16868.396484\n",
      ">>  Epoch: 72 Average loss: -17024.8841\n",
      "Train Epoch: 73 [0/60000 (0%)]\tLoss: -16993.830078\n",
      "Train Epoch: 73 [12800/60000 (21%)]\tLoss: -16626.347656\n",
      "Train Epoch: 73 [25600/60000 (43%)]\tLoss: -16572.027344\n",
      "Train Epoch: 73 [38400/60000 (64%)]\tLoss: -16964.886719\n",
      "Train Epoch: 73 [51200/60000 (85%)]\tLoss: -16832.048828\n",
      ">>  Epoch: 73 Average loss: -16820.0449\n",
      "Train Epoch: 74 [0/60000 (0%)]\tLoss: -16726.564453\n",
      "Train Epoch: 74 [12800/60000 (21%)]\tLoss: -17005.005859\n",
      "Train Epoch: 74 [25600/60000 (43%)]\tLoss: -16827.363281\n",
      "Train Epoch: 74 [38400/60000 (64%)]\tLoss: -16812.699219\n",
      "Train Epoch: 74 [51200/60000 (85%)]\tLoss: -17007.255859\n",
      ">>  Epoch: 74 Average loss: -16855.9665\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: -17013.611328\n",
      "Train Epoch: 75 [12800/60000 (21%)]\tLoss: -16890.156250\n",
      "Train Epoch: 75 [25600/60000 (43%)]\tLoss: -16731.054688\n",
      "Train Epoch: 75 [38400/60000 (64%)]\tLoss: -16648.509766\n",
      "Train Epoch: 75 [51200/60000 (85%)]\tLoss: -16863.269531\n",
      ">>  Epoch: 75 Average loss: -16755.0141\n",
      "Train Epoch: 76 [0/60000 (0%)]\tLoss: -16395.185547\n",
      "Train Epoch: 76 [12800/60000 (21%)]\tLoss: -16981.302734\n",
      "Train Epoch: 76 [25600/60000 (43%)]\tLoss: -16936.773438\n",
      "Train Epoch: 76 [38400/60000 (64%)]\tLoss: -16885.177734\n",
      "Train Epoch: 76 [51200/60000 (85%)]\tLoss: -16895.365234\n",
      ">>  Epoch: 76 Average loss: -16824.4441\n",
      "Train Epoch: 77 [0/60000 (0%)]\tLoss: -16717.771484\n",
      "Train Epoch: 77 [12800/60000 (21%)]\tLoss: -16638.263672\n",
      "Train Epoch: 77 [25600/60000 (43%)]\tLoss: -16863.125000\n",
      "Train Epoch: 77 [38400/60000 (64%)]\tLoss: -16758.496094\n",
      "Train Epoch: 77 [51200/60000 (85%)]\tLoss: -16392.998047\n",
      ">>  Epoch: 77 Average loss: -16704.3087\n",
      "Train Epoch: 78 [0/60000 (0%)]\tLoss: -16794.507812\n",
      "Train Epoch: 78 [12800/60000 (21%)]\tLoss: -16618.425781\n",
      "Train Epoch: 78 [25600/60000 (43%)]\tLoss: -16530.740234\n",
      "Train Epoch: 78 [38400/60000 (64%)]\tLoss: -16447.982422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 78 [51200/60000 (85%)]\tLoss: -16567.976562\n",
      ">>  Epoch: 78 Average loss: -16514.9580\n",
      "Train Epoch: 79 [0/60000 (0%)]\tLoss: -16274.107422\n",
      "Train Epoch: 79 [12800/60000 (21%)]\tLoss: -15965.790039\n",
      "Train Epoch: 79 [25600/60000 (43%)]\tLoss: -16976.404297\n",
      "Train Epoch: 79 [38400/60000 (64%)]\tLoss: -16533.996094\n",
      "Train Epoch: 79 [51200/60000 (85%)]\tLoss: -16301.074219\n",
      ">>  Epoch: 79 Average loss: -16574.6297\n",
      "Train Epoch: 80 [0/60000 (0%)]\tLoss: -16861.246094\n",
      "Train Epoch: 80 [12800/60000 (21%)]\tLoss: -16715.041016\n",
      "Train Epoch: 80 [25600/60000 (43%)]\tLoss: -16621.761719\n",
      "Train Epoch: 80 [38400/60000 (64%)]\tLoss: -16638.947266\n",
      "Train Epoch: 80 [51200/60000 (85%)]\tLoss: -16915.890625\n",
      ">>  Epoch: 80 Average loss: -16756.9602\n",
      "Train Epoch: 81 [0/60000 (0%)]\tLoss: -17538.429688\n",
      "Train Epoch: 81 [12800/60000 (21%)]\tLoss: -17190.730469\n",
      "Train Epoch: 81 [25600/60000 (43%)]\tLoss: -17047.847656\n",
      "Train Epoch: 81 [38400/60000 (64%)]\tLoss: -16756.128906\n",
      "Train Epoch: 81 [51200/60000 (85%)]\tLoss: -16829.742188\n",
      ">>  Epoch: 81 Average loss: -16969.9282\n",
      "Train Epoch: 82 [0/60000 (0%)]\tLoss: -17068.365234\n",
      "Train Epoch: 82 [12800/60000 (21%)]\tLoss: -16725.025391\n",
      "Train Epoch: 82 [25600/60000 (43%)]\tLoss: -16801.292969\n",
      "Train Epoch: 82 [38400/60000 (64%)]\tLoss: -16457.238281\n",
      "Train Epoch: 82 [51200/60000 (85%)]\tLoss: -16701.607422\n",
      ">>  Epoch: 82 Average loss: -16702.8252\n",
      "Train Epoch: 83 [0/60000 (0%)]\tLoss: -17212.253906\n",
      "Train Epoch: 83 [12800/60000 (21%)]\tLoss: -16595.898438\n",
      "Train Epoch: 83 [25600/60000 (43%)]\tLoss: -16469.062500\n",
      "Train Epoch: 83 [38400/60000 (64%)]\tLoss: -16930.515625\n",
      "Train Epoch: 83 [51200/60000 (85%)]\tLoss: -16346.410156\n",
      ">>  Epoch: 83 Average loss: -16681.1220\n",
      "Train Epoch: 84 [0/60000 (0%)]\tLoss: -16859.251953\n",
      "Train Epoch: 84 [12800/60000 (21%)]\tLoss: -16471.759766\n",
      "Train Epoch: 84 [25600/60000 (43%)]\tLoss: -16511.751953\n",
      "Train Epoch: 84 [38400/60000 (64%)]\tLoss: -16661.656250\n",
      "Train Epoch: 84 [51200/60000 (85%)]\tLoss: -16598.855469\n",
      ">>  Epoch: 84 Average loss: -16600.5367\n",
      "Train Epoch: 85 [0/60000 (0%)]\tLoss: -16690.406250\n",
      "Train Epoch: 85 [12800/60000 (21%)]\tLoss: -16376.686523\n",
      "Train Epoch: 85 [25600/60000 (43%)]\tLoss: -17155.906250\n",
      "Train Epoch: 85 [38400/60000 (64%)]\tLoss: -16336.201172\n",
      "Train Epoch: 85 [51200/60000 (85%)]\tLoss: -16828.732422\n",
      ">>  Epoch: 85 Average loss: -16589.3381\n",
      "Train Epoch: 86 [0/60000 (0%)]\tLoss: -16457.542969\n",
      "Train Epoch: 86 [12800/60000 (21%)]\tLoss: -16272.050781\n",
      "Train Epoch: 86 [25600/60000 (43%)]\tLoss: -16127.432617\n",
      "Train Epoch: 86 [38400/60000 (64%)]\tLoss: -16412.441406\n",
      "Train Epoch: 86 [51200/60000 (85%)]\tLoss: -15926.803711\n",
      ">>  Epoch: 86 Average loss: -16343.3976\n",
      "Train Epoch: 87 [0/60000 (0%)]\tLoss: -16248.798828\n",
      "Train Epoch: 87 [12800/60000 (21%)]\tLoss: -16318.914062\n",
      "Train Epoch: 87 [25600/60000 (43%)]\tLoss: -16081.777344\n",
      "Train Epoch: 87 [38400/60000 (64%)]\tLoss: -17063.484375\n",
      "Train Epoch: 87 [51200/60000 (85%)]\tLoss: -16452.720703\n",
      ">>  Epoch: 87 Average loss: -16501.6742\n",
      "Train Epoch: 88 [0/60000 (0%)]\tLoss: -16617.968750\n",
      "Train Epoch: 88 [12800/60000 (21%)]\tLoss: -16762.095703\n",
      "Train Epoch: 88 [25600/60000 (43%)]\tLoss: -16962.550781\n",
      "Train Epoch: 88 [38400/60000 (64%)]\tLoss: -17069.365234\n",
      "Train Epoch: 88 [51200/60000 (85%)]\tLoss: -17031.849609\n",
      ">>  Epoch: 88 Average loss: -16713.8787\n",
      "Train Epoch: 89 [0/60000 (0%)]\tLoss: -16634.357422\n",
      "Train Epoch: 89 [12800/60000 (21%)]\tLoss: -16780.300781\n",
      "Train Epoch: 89 [25600/60000 (43%)]\tLoss: -17194.730469\n",
      "Train Epoch: 89 [38400/60000 (64%)]\tLoss: -16642.835938\n",
      "Train Epoch: 89 [51200/60000 (85%)]\tLoss: -16811.687500\n",
      ">>  Epoch: 89 Average loss: -16672.3675\n",
      "Train Epoch: 90 [0/60000 (0%)]\tLoss: -16412.855469\n",
      "Train Epoch: 90 [12800/60000 (21%)]\tLoss: -16578.832031\n",
      "Train Epoch: 90 [25600/60000 (43%)]\tLoss: -16710.269531\n",
      "Train Epoch: 90 [38400/60000 (64%)]\tLoss: -16771.345703\n",
      "Train Epoch: 90 [51200/60000 (85%)]\tLoss: -16795.388672\n",
      ">>  Epoch: 90 Average loss: -16634.6537\n",
      "Train Epoch: 91 [0/60000 (0%)]\tLoss: -16775.830078\n",
      "Train Epoch: 91 [12800/60000 (21%)]\tLoss: -16958.330078\n",
      "Train Epoch: 91 [25600/60000 (43%)]\tLoss: -16458.888672\n",
      "Train Epoch: 91 [38400/60000 (64%)]\tLoss: -16233.967773\n",
      "Train Epoch: 91 [51200/60000 (85%)]\tLoss: -16599.865234\n",
      ">>  Epoch: 91 Average loss: -16741.0825\n",
      "Train Epoch: 92 [0/60000 (0%)]\tLoss: -17074.335938\n",
      "Train Epoch: 92 [12800/60000 (21%)]\tLoss: -16677.009766\n",
      "Train Epoch: 92 [25600/60000 (43%)]\tLoss: -16861.306641\n",
      "Train Epoch: 92 [38400/60000 (64%)]\tLoss: -16204.630859\n",
      "Train Epoch: 92 [51200/60000 (85%)]\tLoss: -16753.148438\n",
      ">>  Epoch: 92 Average loss: -16639.2805\n",
      "Train Epoch: 93 [0/60000 (0%)]\tLoss: -16560.656250\n",
      "Train Epoch: 93 [12800/60000 (21%)]\tLoss: -17046.308594\n",
      "Train Epoch: 93 [25600/60000 (43%)]\tLoss: -16815.138672\n",
      "Train Epoch: 93 [38400/60000 (64%)]\tLoss: -16509.044922\n",
      "Train Epoch: 93 [51200/60000 (85%)]\tLoss: -16683.046875\n",
      ">>  Epoch: 93 Average loss: -16743.7790\n",
      "Train Epoch: 94 [0/60000 (0%)]\tLoss: -16674.994141\n",
      "Train Epoch: 94 [12800/60000 (21%)]\tLoss: -16944.917969\n",
      "Train Epoch: 94 [25600/60000 (43%)]\tLoss: -16855.167969\n",
      "Train Epoch: 94 [38400/60000 (64%)]\tLoss: -16546.781250\n",
      "Train Epoch: 94 [51200/60000 (85%)]\tLoss: -16807.929688\n",
      ">>  Epoch: 94 Average loss: -16767.7904\n",
      "Train Epoch: 95 [0/60000 (0%)]\tLoss: -17421.816406\n",
      "Train Epoch: 95 [12800/60000 (21%)]\tLoss: -16699.886719\n",
      "Train Epoch: 95 [25600/60000 (43%)]\tLoss: -16530.775391\n",
      "Train Epoch: 95 [38400/60000 (64%)]\tLoss: -16462.689453\n",
      "Train Epoch: 95 [51200/60000 (85%)]\tLoss: -16422.068359\n",
      ">>  Epoch: 95 Average loss: -16710.3526\n",
      "Train Epoch: 96 [0/60000 (0%)]\tLoss: -17130.708984\n",
      "Train Epoch: 96 [12800/60000 (21%)]\tLoss: -16741.439453\n",
      "Train Epoch: 96 [25600/60000 (43%)]\tLoss: -16863.515625\n",
      "Train Epoch: 96 [38400/60000 (64%)]\tLoss: -16242.845703\n",
      "Train Epoch: 96 [51200/60000 (85%)]\tLoss: -16575.855469\n",
      ">>  Epoch: 96 Average loss: -16707.6568\n",
      "Train Epoch: 97 [0/60000 (0%)]\tLoss: -16601.457031\n",
      "Train Epoch: 97 [12800/60000 (21%)]\tLoss: -16559.931641\n",
      "Train Epoch: 97 [25600/60000 (43%)]\tLoss: -16509.224609\n",
      "Train Epoch: 97 [38400/60000 (64%)]\tLoss: -16780.644531\n",
      "Train Epoch: 97 [51200/60000 (85%)]\tLoss: -16513.462891\n",
      ">>  Epoch: 97 Average loss: -16538.1087\n",
      "Train Epoch: 98 [0/60000 (0%)]\tLoss: -17080.683594\n",
      "Train Epoch: 98 [12800/60000 (21%)]\tLoss: -16576.988281\n",
      "Train Epoch: 98 [25600/60000 (43%)]\tLoss: -16717.597656\n",
      "Train Epoch: 98 [38400/60000 (64%)]\tLoss: -16582.261719\n",
      "Train Epoch: 98 [51200/60000 (85%)]\tLoss: -16598.505859\n",
      ">>  Epoch: 98 Average loss: -16717.3681\n",
      "Train Epoch: 99 [0/60000 (0%)]\tLoss: -16601.992188\n",
      "Train Epoch: 99 [12800/60000 (21%)]\tLoss: -16578.425781\n",
      "Train Epoch: 99 [25600/60000 (43%)]\tLoss: -16143.469727\n",
      "Train Epoch: 99 [38400/60000 (64%)]\tLoss: -16556.958984\n",
      "Train Epoch: 99 [51200/60000 (85%)]\tLoss: -16283.593750\n",
      ">>  Epoch: 99 Average loss: -16547.9954\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(train_dl):\n",
    "        x = x.view(x.size(0), -1) # batchsize x 128\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        \n",
    "        y_hat, mu, logvar = model(x)\n",
    "        \n",
    "        loss = loss_f(y_hat, x, mu, logvar)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()                        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data.item()        \n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(x),\n",
    "                len(train_dl.dataset), 100. * batch_idx / len(train_dl),\n",
    "                loss.data.item() / len(x)))\n",
    "\n",
    "    print('>>  Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_dl.dataset)))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        save_image(toImg(x.cpu().data), '../results/01-autoencoder/vae/image_{}_i.png'.format(epoch))       \n",
    "        save_image(toImg(y_hat.cpu().data), '../results/01-autoencoder/vae/image_{}_o.png'.format(epoch))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved as: ./saved-mdls/01-autoencoder/vae_100e.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type VAE. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# As suggested in https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "filename = '../saved-mdls/01-autoencoder/vae_{}e.pt'.format(epoch+1)\n",
    "torch.save(model, filename)\n",
    "\n",
    "print('model saved as: {}'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreamunafo/opt/anaconda3/envs/compiling-ai/lib/python3.6/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/andreamunafo/opt/anaconda3/envs/compiling-ai/lib/python3.6/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Sigmoid' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (base_encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (mean_encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Linear(in_features=400, out_features=20, bias=True)\n",
       "  )\n",
       "  (var_encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Linear(in_features=400, out_features=20, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=400, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=400, out_features=784, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model class must be defined somewhere\n",
    "model = torch.load('./saved-mdls/01-autoencoder/vae_100e.pt',\n",
    "                  map_location=torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the transforms to normalise the image (as done during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pic mean: 0.0921572670340538, std: 1.0990101099014282\n",
      "\n",
      "pic shape (C x H x W): torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Define the transforms to normalise the image\n",
    "pic_tt = transforms.ToTensor() # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "pic_n = transforms.Normalize((ds_mean,), (ds_std,))\n",
    "\n",
    "if torch.__version__ != '1.3.0':\n",
    "    pic_data = valid_ds.test_data[idx].numpy()\n",
    "    pic_data = pic_data[:, :, None] # (H x W x C)\n",
    "    print('pic_data.dtype: {}'.format(pic_data.dtype))\n",
    "else:\n",
    "    pic_data = valid_ds.data[idx].numpy()\n",
    "    \n",
    "pic = pic_tt(pic_data)\n",
    "pic = pic_n(pic)\n",
    "\n",
    "# print stats\n",
    "print('pic mean: {}, std: {}'.format(pic.mean(), pic.std()))\n",
    "\n",
    "# print pic shape\n",
    "print('\\npic shape (C x H x W): {}'.format(pic.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x120d4f1d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pic.view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "# check size\n",
    "print(pic.to(device)[None, :, :, :].shape)\n",
    "print(pic.to(device).view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded mean: [[-0.5592429   1.4444453   1.1667817  -0.18782139  1.0589974  -0.55701214\n",
      "  -0.07533638  0.1629481   0.64944714  5.653008    0.3813714  -0.9495977\n",
      "   1.085453   -1.5471504  -3.5718756   1.2370685  -0.7264838   0.53405297\n",
      "   1.5231957  -1.6848253 ]]\n",
      "encoded  var: [[-5.571127  -6.57798   -6.2529154 -5.8869295 -5.8825827 -5.62205\n",
      "  -5.6434155 -6.084442  -6.771547  -5.3372474 -5.7165003 -6.550778\n",
      "  -5.552048  -6.1818843 -5.855712  -5.3861585 -6.335955  -6.6406946\n",
      "  -6.5768113 -6.869463 ]]\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "mean_encoding, var_encoding = model.encode(pic.to(device).view(1, -1)) # note that we need to reshape the pic to be: bsx28*28 \n",
    "\n",
    "# print encoding result\n",
    "print('encoded mean: {}'.format(mean_encoding.detach().cpu().numpy()))\n",
    "print('encoded  var: {}'.format(var_encoding.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding, _, _ = model.decode((mean_encoding, var_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x120eb0358>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(decoding.cpu().data.view(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
